{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $$Processamento~ de~ linguagem ~natural$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $$Conceito:$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PLN é um ramo da Inteligência Artificial (IA) que tem como objetivo dar aos dispositivos tecnológicos a capacidade de entender, responder e interpretar a linguagem humana, seja por meio de áudio ou texto\n",
    "\n",
    "\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### $$Caracteristicas:$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Mineração de Texto (Text Mining)** = O processo consiste em criar datasets de documentos (chamados ‘corpus’) e transformar a informação contida nos mesmos em uma matriz numérica.\n",
    "\n",
    "\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Corpus** = Um Corpus é uma coleção de arquivos de texto organizados em um diretório que conseguimos manipulá-los todos de uma vez. Ex: Coletamos dados de textos de vários sites, organizamos tudo dentro de um Corpus e conseguimos manipulá-los todos os arquivos como se fosse 1 só.\n",
    "\n",
    "\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Token** = Estruturas mínimas dentro do texto, dividir um texto em pedaços menores (Frase, palavras, n-grama)\n",
    "\n",
    "\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Bag of words** = É um modelo de linguagem estatística usado para analisar texto e documentos com base na contagem de palavras, verifica a frequência dos termos e conta os mesmos\n",
    "\n",
    "\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Text Cleaning** = Processo para converter os textos em um formato consistente. O texto será limpo, EX: Lowercase / UpperCase, Punctuation Removal, Removing Stop Words, Stemming, Lemmatization, Tokenization e convertido em uma matriz \n",
    "\n",
    "\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Stop Word** = São palavras comuns que normalmente não contribuem para o significado de uma frase, Ex:('de','a','o','que','e','é','do','da','em','um')\n",
    "\n",
    "\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Derivação (Stemming)** = É a técnica de remover sufixos e prefixos de uma palavra, chamada stem(Base), Exemplo, o stem(Base) da palavra cooking - cook, amigos - amig, amizade - amizad\n",
    "\n",
    "\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Lematização (Lemmatization)** = Reduzir as formas flexionadas, derivadas de uma palavra para uma base comum, considerando a estrutura formologica, Ex: Organizar, Organizado e Organizando para Organiza, é o ato de representar as palavras através do infinitivo dos verbos e masculino singular dos substantivos e adjetivos.\n",
    "\n",
    "\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Pairwise Correlation** = Identificar grupos de características altamente correlacionadas e apenas manter um deles para que seu modelo possa ter o máximo de poder preditivo usando o menor número possível de recursos.\n",
    "\n",
    "\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Part-of-Speech Tag** = Retorna a classe gramatical de cada palavra tokenizada (Adjetivo, substantivo, verbo,etc.)\n",
    "\n",
    "\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Collocations** = São duas ou mais palavras que tendem a aparecer frequentemente juntas, como \"Estados Unidos\", \"Rio Grande do Sul\" ou \"Machine Learning\". Essas palavras podem gerar diversas combinações por isso o contexto também é importante no processamento de linguagem natural.\n",
    "\n",
    "\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### $$Expressões ~Regulares:$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Fazer uma busca no texto definindo algum padrão, podendo ser:**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Match:** verifica uma correspondência apenas no início da sequência de caracteres\n",
    "\n",
    "\n",
    "\n",
    "**Search:** verifica uma correspondência em qualquer parte da sequência de caracteres.\n",
    "\n",
    "\n",
    "\n",
    "**[A-z]:** Filtra as letras de A até Z, maiúsculas ou minúsculas em um texto\n",
    "\n",
    "\n",
    "\n",
    "**[0-9]:** Filtra somente os números do texto\n",
    "\n",
    "\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### $$Token ~n-grama:$$ \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Dada uma sequência de palavras N-1, um modelo N-grama prevê a palavra mais provável que pode seguir esta sequência\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Um modelo que simplesmente se baseia na frequência com que uma palavra ocorre sem olhar para palavras anteriores é chamado de **unigrama**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Se um modelo considera apenas a palavra anterior para prever a palavra atual, então ela é chamada de **bigram**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Se duas palavras anteriores forem consideradas, então é um modelo de **trigrama**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Exemplos:**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- **Unigrama** = 'ele', 'Guilherme', 'Casa'\n",
    "\n",
    "\n",
    "\n",
    "- **Bigrama** = 'Eu fui', 'Mão pequena', 'Garota bonita'\n",
    "\n",
    "\n",
    "\n",
    "- **Trigrama** = 'Eu a levei, 'Cachorro é grande', 'Fui ao parque'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------__\n",
    "\n",
    "\n",
    "__------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $$Objetivo:$$\n",
    "\n",
    "## $$Compreender~ os~ principais~ conceitos~ de~ PLN$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------__\n",
    "\n",
    "\n",
    "__------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bibliotecas\n",
    "import nltk\n",
    "import spacy\n",
    "import requests\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "import json\n",
    "import pandas as pd\n",
    "import string\n",
    "import spacy\n",
    "import random\n",
    "import seaborn as sns\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus: Str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Desejo que você não tenha medo da vida, tenha medo de não vivê-la. Não há céu sem tempestades, nem caminhos sem acidentes.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Frase = \"Desejo que você não tenha medo da vida, tenha medo de não vivê-la. Não há céu sem tempestades, nem caminhos sem acidentes.\"\n",
    "Frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#formato do dado\n",
    "type(Frase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Corpus no formato STR tem o início e o fim entre Aspas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizar em sentenças"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Desejo que você não tenha medo da vida, tenha medo de não vivê-la.',\n",
       " 'Não há céu sem tempestades, nem caminhos sem acidentes.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizar a frase em sentenças\n",
    "from nltk.tokenize import sent_tokenize\n",
    "token1 = sent_tokenize(Frase)\n",
    "token1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Desejo que você não tenha medo da vida, tenha medo de não vivê-la.',\n",
       " 'Não há céu sem tempestades, nem caminhos sem acidentes.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizar a frase em sentenças, utilizando dicionário do pacote NLTK\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/portuguese.pickle')\n",
    "token2 = tokenizer.tokenize(Frase)\n",
    "token2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Veja que a frase que era unica, foi tokenizada em 2 sentenças**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "#formato dos dados\n",
    "print(type(token1))\n",
    "print(type(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizando a frase em sentenças, o formato dos dados ficam como lista**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizar em palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Desejo',\n",
       " 'que',\n",
       " 'você',\n",
       " 'não',\n",
       " 'tenha',\n",
       " 'medo',\n",
       " 'da',\n",
       " 'vida',\n",
       " ',',\n",
       " 'tenha',\n",
       " 'medo',\n",
       " 'de',\n",
       " 'não',\n",
       " 'vivê-la.',\n",
       " 'Não',\n",
       " 'há',\n",
       " 'céu',\n",
       " 'sem',\n",
       " 'tempestades',\n",
       " ',',\n",
       " 'nem',\n",
       " 'caminhos',\n",
       " 'sem',\n",
       " 'acidentes',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenizar as palavras\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "token3 = tokenizer.tokenize(Frase)\n",
    "token3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Desejo',\n",
       "  'que',\n",
       "  'você',\n",
       "  'não',\n",
       "  'tenha',\n",
       "  'medo',\n",
       "  'da',\n",
       "  'vida',\n",
       "  ',',\n",
       "  'tenha',\n",
       "  'medo',\n",
       "  'de',\n",
       "  'não',\n",
       "  'vivê-la',\n",
       "  '.'],\n",
       " ['Não',\n",
       "  'há',\n",
       "  'céu',\n",
       "  'sem',\n",
       "  'tempestades',\n",
       "  ',',\n",
       "  'nem',\n",
       "  'caminhos',\n",
       "  'sem',\n",
       "  'acidentes',\n",
       "  '.']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizar as palavras com List Comprehension\n",
    "token4 = ([word_tokenize(palavra) for palavra in sent_tokenize(Frase)])\n",
    "token4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizar as frases em palavras, cada palavra gera um token**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "#formato dos dados\n",
    "print(type(token3))\n",
    "print(type(token4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remover pontuações de uma frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Desejo que você não tenha medo da vida tenha medo de não vivêla Não há céu sem tempestades nem caminhos sem acidentes'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remov_str = Frase.translate(str.maketrans('', '', string.punctuation))\n",
    "remov_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#formato dos dados\n",
    "type(remov_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remover pontuações de uma frase e tokeniza-la em palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Desejo',\n",
       " 'que',\n",
       " 'você',\n",
       " 'não',\n",
       " 'tenha',\n",
       " 'medo',\n",
       " 'da',\n",
       " 'vida',\n",
       " 'tenha',\n",
       " 'medo',\n",
       " 'de',\n",
       " 'não',\n",
       " 'vivê',\n",
       " 'la',\n",
       " 'Não',\n",
       " 'há',\n",
       " 'céu',\n",
       " 'sem',\n",
       " 'tempestades',\n",
       " 'nem',\n",
       " 'caminhos',\n",
       " 'sem',\n",
       " 'acidentes']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remover as pontuações\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "#Frase do tipo str\n",
    "token_sempont = tokenizer.tokenize(Frase)\n",
    "token_sempont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note que só há palavras sem pontuações**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remover Stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Desejo',\n",
       " 'medo',\n",
       " 'vida',\n",
       " 'medo',\n",
       " 'vivê',\n",
       " 'la',\n",
       " 'Não',\n",
       " 'céu',\n",
       " 'tempestades',\n",
       " 'caminhos',\n",
       " 'acidentes']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "#Já precisa estar tokenizado para retirar as stopwords\n",
    "portuguese_stops = set(stopwords.words('portuguese'))\n",
    "\n",
    "\n",
    "sem_stop = [palavra for palavra in token_sempont if palavra not in portuguese_stops]\n",
    "sem_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Diminuímos ainda mais o texto removendo as stopwords**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words - Contagem de palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('não', 2),\n",
       " ('tenha', 2),\n",
       " ('medo', 2),\n",
       " ('sem', 2),\n",
       " ('Desejo', 1),\n",
       " ('que', 1),\n",
       " ('você', 1),\n",
       " ('da', 1),\n",
       " ('vida', 1),\n",
       " ('de', 1),\n",
       " ('vivê', 1),\n",
       " ('la', 1),\n",
       " ('Não', 1),\n",
       " ('há', 1),\n",
       " ('céu', 1),\n",
       " ('tempestades', 1),\n",
       " ('nem', 1),\n",
       " ('caminhos', 1),\n",
       " ('acidentes', 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "freq = FreqDist(token_sempont)\n",
    "freq.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observe que o número representa a quantidade de vozes que a palavra aparece no texto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'desejo': 1,\n",
       "             'que': 1,\n",
       "             'você': 1,\n",
       "             'não': 3,\n",
       "             'tenha': 2,\n",
       "             'medo': 2,\n",
       "             'da': 1,\n",
       "             'vida': 1,\n",
       "             'de': 1,\n",
       "             'vivê-la': 1,\n",
       "             'há': 1,\n",
       "             'céu': 1,\n",
       "             'sem': 2,\n",
       "             'tempestades': 1,\n",
       "             'nem': 1,\n",
       "             'caminhos': 1,\n",
       "             'acidentes': 1})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Outra forma de usar Bag of Words\n",
    "from textblob import TextBlob\n",
    "blob = TextBlob(Frase)\n",
    "\n",
    "blob.word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicar o Stemming nas palavras tokenizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['desej',\n",
       " 'que',\n",
       " 'voc',\n",
       " 'não',\n",
       " 'tenh',\n",
       " 'med',\n",
       " 'da',\n",
       " 'vid',\n",
       " 'tenh',\n",
       " 'med',\n",
       " 'de',\n",
       " 'não',\n",
       " 'viv',\n",
       " 'la',\n",
       " 'não',\n",
       " 'há',\n",
       " 'céu',\n",
       " 'sem',\n",
       " 'tempestad',\n",
       " 'nem',\n",
       " 'caminh',\n",
       " 'sem',\n",
       " 'acid']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stemming_bag(token_sempont, stemmer):\n",
    "    return [stemmer.stem(w) for w in token_sempont]\n",
    "\n",
    "stemming_bag(token_sempont, nltk.stem.RSLPStemmer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Desejo', 'desej'), ('que', 'que'), ('você', 'voc'), ('não', 'não'), ('tenha', 'tenh'), ('medo', 'med'), ('da', 'da'), ('vida', 'vid'), ('tenha', 'tenh'), ('medo', 'med'), ('de', 'de'), ('não', 'não'), ('vivê', 'viv'), ('la', 'la'), ('Não', 'não'), ('há', 'há'), ('céu', 'céu'), ('sem', 'sem'), ('tempestades', 'tempestad'), ('nem', 'nem'), ('caminhos', 'caminh'), ('sem', 'sem'), ('acidentes', 'acid')]\n"
     ]
    }
   ],
   "source": [
    "#Comparação antes e depois \n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "\n",
    "def stemming_bag(token_sempont, stemmer):\n",
    "    return [stemmer.stem(w) for w in token_sempont]\n",
    "\n",
    "\n",
    "def imprime(w, result):\n",
    "    return list(zip(w, result))\n",
    "\n",
    "\n",
    "rslp = stemming_bag(token_sempont, nltk.stem.RSLPStemmer())\n",
    "print(imprime(token_sempont,rslp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparação das palavras antes e depois do stemming**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lematização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Desejo que você não tenha medo da vida, tenha medo de não vivê-la. Não há céu sem tempestades, nem caminhos sem acidentes."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Converter STR para spacy.tokens.doc\n",
    "import spacy\n",
    "from spacy.lang.pt.examples import sentences\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "\n",
    "doc = nlp(Frase)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tipo de dado\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desejo\n",
      "que\n",
      "você\n",
      "não\n",
      "ter\n",
      "medo\n",
      "de o\n",
      "vida\n",
      ",\n",
      "ter\n",
      "medo\n",
      "de\n",
      "não\n",
      "vivê-la\n",
      ".\n",
      "não\n",
      "haver\n",
      "céu\n",
      "sem\n",
      "tempestade\n",
      ",\n",
      "nem\n",
      "caminho\n",
      "sem\n",
      "acidente\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "#Aplicar lematização\n",
    "for token in doc:\n",
    "    print(token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desejo | Desejo\n",
      "que | que\n",
      "você | você\n",
      "não | não\n",
      "tenha | ter\n",
      "medo | medo\n",
      "da | de o\n",
      "vida | vida\n",
      ", | ,\n",
      "tenha | ter\n",
      "medo | medo\n",
      "de | de\n",
      "não | não\n",
      "vivê-la | vivê-la\n",
      ". | .\n",
      "Não | não\n",
      "há | haver\n",
      "céu | céu\n",
      "sem | sem\n",
      "tempestades | tempestade\n",
      ", | ,\n",
      "nem | nem\n",
      "caminhos | caminho\n",
      "sem | sem\n",
      "acidentes | acidente\n",
      ". | .\n"
     ]
    }
   ],
   "source": [
    "#Antes e depois da lematização\n",
    "for token in doc:\n",
    "    print(token.text,\"|\",token.lemma_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparação das palavras antes e depois da Lematização**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming X Lematização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desejo | desej | Desejo\n",
      "que | que | que\n",
      "você | voc | você\n",
      "não | não | não\n",
      "tenha | tenh | ter\n",
      "medo | med | medo\n",
      "da | da | de o\n",
      "vida | vid | vida\n",
      ", | , | ,\n",
      "tenha | tenh | ter\n",
      "medo | med | medo\n",
      "de | de | de\n",
      "não | não | não\n",
      "vivê-la | vivê-l | vivê-la\n",
      ". | . | .\n",
      "Não | não | não\n",
      "há | há | haver\n",
      "céu | céu | céu\n",
      "sem | sem | sem\n",
      "tempestades | tempestad | tempestade\n",
      ", | , | ,\n",
      "nem | nem | nem\n",
      "caminhos | caminh | caminho\n",
      "sem | sem | sem\n",
      "acidentes | acid | acidente\n",
      ". | . | .\n"
     ]
    }
   ],
   "source": [
    "#Comparação entre Lematização e Stemming\n",
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "\n",
    "\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text,\"|\",stemmer.stem(token.text),\"|\",token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Podemos notar a diferença entre a lematização e o Stemming**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lower() e upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'desejo que você não tenha medo da vida, tenha medo de não vivê-la. não há céu sem tempestades, nem caminhos sem acidentes.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Frase.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DESEJO QUE VOCÊ NÃO TENHA MEDO DA VIDA, TENHA MEDO DE NÃO VIVÊ-LA. NÃO HÁ CÉU SEM TEMPESTADES, NEM CAMINHOS SEM ACIDENTES.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Frase.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deixar as palavras com caixa baixa e alta**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Desejo', 'que'),\n",
       " ('que', 'você'),\n",
       " ('você', 'não'),\n",
       " ('não', 'tenha'),\n",
       " ('tenha', 'medo'),\n",
       " ('medo', 'da'),\n",
       " ('da', 'vida'),\n",
       " ('vida', ','),\n",
       " (',', 'tenha'),\n",
       " ('tenha', 'medo'),\n",
       " ('medo', 'de'),\n",
       " ('de', 'não'),\n",
       " ('não', 'vivê-la.'),\n",
       " ('vivê-la.', 'Não'),\n",
       " ('Não', 'há'),\n",
       " ('há', 'céu'),\n",
       " ('céu', 'sem'),\n",
       " ('sem', 'tempestades'),\n",
       " ('tempestades', ','),\n",
       " (',', 'nem'),\n",
       " ('nem', 'caminhos'),\n",
       " ('caminhos', 'sem'),\n",
       " ('sem', 'acidentes'),\n",
       " ('acidentes', '.')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bigramas\n",
    "\n",
    "from nltk import bigrams\n",
    "list(bigrams(token3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sequência de duas letras consecutivas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Desejo', 'que', 'você'),\n",
       " ('que', 'você', 'não'),\n",
       " ('você', 'não', 'tenha'),\n",
       " ('não', 'tenha', 'medo'),\n",
       " ('tenha', 'medo', 'da'),\n",
       " ('medo', 'da', 'vida'),\n",
       " ('da', 'vida', ','),\n",
       " ('vida', ',', 'tenha'),\n",
       " (',', 'tenha', 'medo'),\n",
       " ('tenha', 'medo', 'de'),\n",
       " ('medo', 'de', 'não'),\n",
       " ('de', 'não', 'vivê-la.'),\n",
       " ('não', 'vivê-la.', 'Não'),\n",
       " ('vivê-la.', 'Não', 'há'),\n",
       " ('Não', 'há', 'céu'),\n",
       " ('há', 'céu', 'sem'),\n",
       " ('céu', 'sem', 'tempestades'),\n",
       " ('sem', 'tempestades', ','),\n",
       " ('tempestades', ',', 'nem'),\n",
       " (',', 'nem', 'caminhos'),\n",
       " ('nem', 'caminhos', 'sem'),\n",
       " ('caminhos', 'sem', 'acidentes'),\n",
       " ('sem', 'acidentes', '.')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Trigramas\n",
    "\n",
    "from nltk import trigrams\n",
    "list(trigrams(token3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sequência de três letras consecutivas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['Desejo', 'que', 'você', 'não']),\n",
       " WordList(['que', 'você', 'não', 'tenha']),\n",
       " WordList(['você', 'não', 'tenha', 'medo']),\n",
       " WordList(['não', 'tenha', 'medo', 'da']),\n",
       " WordList(['tenha', 'medo', 'da', 'vida']),\n",
       " WordList(['medo', 'da', 'vida', 'tenha']),\n",
       " WordList(['da', 'vida', 'tenha', 'medo']),\n",
       " WordList(['vida', 'tenha', 'medo', 'de']),\n",
       " WordList(['tenha', 'medo', 'de', 'não']),\n",
       " WordList(['medo', 'de', 'não', 'vivê-la']),\n",
       " WordList(['de', 'não', 'vivê-la', 'Não']),\n",
       " WordList(['não', 'vivê-la', 'Não', 'há']),\n",
       " WordList(['vivê-la', 'Não', 'há', 'céu']),\n",
       " WordList(['Não', 'há', 'céu', 'sem']),\n",
       " WordList(['há', 'céu', 'sem', 'tempestades']),\n",
       " WordList(['céu', 'sem', 'tempestades', 'nem']),\n",
       " WordList(['sem', 'tempestades', 'nem', 'caminhos']),\n",
       " WordList(['tempestades', 'nem', 'caminhos', 'sem']),\n",
       " WordList(['nem', 'caminhos', 'sem', 'acidentes'])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Quadrigrama\n",
    "\n",
    "from textblob import TextBlob\n",
    "bob = TextBlob(Frase)\n",
    "bob.ngrams(n=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sequência de Quatro letras consecutivas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-Speech Tag - Morfologia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Desejo', 'NPROP'), ('que', 'PRO-KS-REL'), ('você', 'PROPESS'), ('não', 'ADV'), ('tenha', 'VAUX'), ('medo', 'N'), ('da', 'NPROP'), ('vida', 'N'), (',', ','), ('tenha', 'VAUX'), ('medo', 'N'), ('de', 'PREP'), ('não', 'ADV'), ('vivê-la', None), ('.', '.'), ('Não', 'ADV'), ('há', 'V'), ('céu', 'N'), ('sem', 'PREP'), ('tempestades', 'N'), (',', ','), ('nem', 'KC'), ('caminhos', 'N'), ('sem', 'PREP'), ('acidentes', 'N'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import mac_morpho\n",
    "from nltk.tag import UnigramTagger\n",
    "\n",
    "tokens = nltk.word_tokenize(Frase)\n",
    "sent_train = mac_morpho.tagged_sents()\n",
    "etiq = UnigramTagger(sent_train)\n",
    "tags = etiq.tag(tokens)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Desejo', 'NNP'),\n",
       " ('que', 'NN'),\n",
       " ('você', 'NN'),\n",
       " ('não', 'JJ'),\n",
       " ('tenha', 'NN'),\n",
       " ('medo', 'NN'),\n",
       " ('da', 'NN'),\n",
       " ('vida', 'NN'),\n",
       " ('tenha', 'JJ'),\n",
       " ('medo', 'FW'),\n",
       " ('de', 'FW'),\n",
       " ('não', 'FW'),\n",
       " ('vivê-la', 'NN'),\n",
       " ('Não', 'NNP'),\n",
       " ('há', 'NN'),\n",
       " ('céu', 'NN'),\n",
       " ('sem', 'NN'),\n",
       " ('tempestades', 'NNS'),\n",
       " ('nem', 'JJ'),\n",
       " ('caminhos', 'NN'),\n",
       " ('sem', 'NN'),\n",
       " ('acidentes', 'NNS')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Outra forma de verificar a Morfologia das palavras\n",
    "from textblob import TextBlob\n",
    "morf = TextBlob(Frase)\n",
    "\n",
    "morf.tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Morfologia das palavras**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus: Lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O sucesso tem uma estranha capacidade de esconder o erro.',\n",
       " 'Só existe um êxito: a capacidade de levar a vida que se quer.',\n",
       " 'A coragem não é ausência do medo; é a persistência apesar do medo.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Texto no formato de lista\n",
    "text = [\"O sucesso tem uma estranha capacidade de esconder o erro.\",\n",
    "      \"Só existe um êxito: a capacidade de levar a vida que se quer.\",\n",
    "      \"A coragem não é ausência do medo; é a persistência apesar do medo.\"]\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#formato dos dados\n",
    "type(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Corpus no formato lista tem o início e o fim entre colchetes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformar Lista em STR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['O sucesso tem uma estranha capacidade de esconder o erro.', 'Só existe um êxito: a capacidade de levar a vida que se quer.', 'A coragem não é ausência do medo; é a persistência apesar do medo.']\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lista -> STR\n",
    "\n",
    "new_text = str(text)\n",
    "new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note que para transformar uma 'lista' em 'Str', foi adicionado aspas no começo e fim da frase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apesar',\n",
       " 'ausência',\n",
       " 'capacidade',\n",
       " 'coragem',\n",
       " 'de',\n",
       " 'do',\n",
       " 'erro',\n",
       " 'esconder',\n",
       " 'estranha',\n",
       " 'existe',\n",
       " 'levar',\n",
       " 'medo',\n",
       " 'não',\n",
       " 'persistência',\n",
       " 'que',\n",
       " 'quer',\n",
       " 'se',\n",
       " 'sucesso',\n",
       " 'só',\n",
       " 'tem',\n",
       " 'um',\n",
       " 'uma',\n",
       " 'vida',\n",
       " 'êxito']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Criar um vetor de palavras unicas (Tokenizar) e remover stopwords \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "vectorizer_text = CountVectorizer(min_df=1,ngram_range=(1,1))\n",
    "\n",
    "term_text_matriz = vectorizer_text.fit_transform(text)\n",
    "\n",
    "vectorizer_text = vectorizer_text.get_feature_names()\n",
    "vectorizer_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frases</th>\n",
       "      <th>apesar do</th>\n",
       "      <th>ausência do</th>\n",
       "      <th>capacidade de</th>\n",
       "      <th>coragem não</th>\n",
       "      <th>de esconder</th>\n",
       "      <th>de levar</th>\n",
       "      <th>do medo</th>\n",
       "      <th>esconder erro</th>\n",
       "      <th>estranha capacidade</th>\n",
       "      <th>...</th>\n",
       "      <th>persistência apesar</th>\n",
       "      <th>que se</th>\n",
       "      <th>se quer</th>\n",
       "      <th>sucesso tem</th>\n",
       "      <th>só existe</th>\n",
       "      <th>tem uma</th>\n",
       "      <th>um êxito</th>\n",
       "      <th>uma estranha</th>\n",
       "      <th>vida que</th>\n",
       "      <th>êxito capacidade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O sucesso tem uma estranha capacidade de escon...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Só existe um êxito: a capacidade de levar a vi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A coragem não é ausência do medo; é a persistê...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Frases  apesar do  ausência do  \\\n",
       "0  O sucesso tem uma estranha capacidade de escon...          0            0   \n",
       "1  Só existe um êxito: a capacidade de levar a vi...          0            0   \n",
       "2  A coragem não é ausência do medo; é a persistê...          1            1   \n",
       "\n",
       "   capacidade de  coragem não  de esconder  de levar  do medo  esconder erro  \\\n",
       "0              1            0            1         0        0              1   \n",
       "1              1            0            0         1        0              0   \n",
       "2              0            1            0         0        2              0   \n",
       "\n",
       "   estranha capacidade  ...  persistência apesar  que se  se quer  \\\n",
       "0                    1  ...                    0       0        0   \n",
       "1                    0  ...                    0       1        1   \n",
       "2                    0  ...                    1       0        0   \n",
       "\n",
       "   sucesso tem  só existe  tem uma  um êxito  uma estranha  vida que  \\\n",
       "0            1          0        1         0             1         0   \n",
       "1            0          1        0         1             0         1   \n",
       "2            0          0        0         0             0         0   \n",
       "\n",
       "   êxito capacidade  \n",
       "0                 0  \n",
       "1                 1  \n",
       "2                 0  \n",
       "\n",
       "[3 rows x 24 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Criar uma matriz de palavras\n",
    "text_matriz = pd.DataFrame(term_text_matriz.toarray(), columns=vectorizer_text)\n",
    "\n",
    "\n",
    "#Adicionando as frases na matriz de palavras\n",
    "text_df = pd.DataFrame(text,columns=['Frases'])\n",
    "result = pd.concat([text_df,text_matriz], axis=1)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observe que na matriz de palavras o valor do número é referente a quantidade de vezes que a palavra aparece no texto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apesar do',\n",
       " 'ausência do',\n",
       " 'capacidade de',\n",
       " 'coragem não',\n",
       " 'de esconder',\n",
       " 'de levar',\n",
       " 'do medo',\n",
       " 'esconder erro',\n",
       " 'estranha capacidade',\n",
       " 'existe um',\n",
       " 'levar vida',\n",
       " 'medo persistência',\n",
       " 'não ausência',\n",
       " 'persistência apesar',\n",
       " 'que se',\n",
       " 'se quer',\n",
       " 'sucesso tem',\n",
       " 'só existe',\n",
       " 'tem uma',\n",
       " 'um êxito',\n",
       " 'uma estranha',\n",
       " 'vida que',\n",
       " 'êxito capacidade']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Criar um vetor de palavras com bigramas\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "vectorizer_text = CountVectorizer(min_df=1,ngram_range=(2,2))\n",
    "\n",
    "term_text_matriz = vectorizer_text.fit_transform(text)\n",
    "\n",
    "vectorizer_text = vectorizer_text.get_feature_names()\n",
    "vectorizer_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frases</th>\n",
       "      <th>apesar do</th>\n",
       "      <th>ausência do</th>\n",
       "      <th>capacidade de</th>\n",
       "      <th>coragem não</th>\n",
       "      <th>de esconder</th>\n",
       "      <th>de levar</th>\n",
       "      <th>do medo</th>\n",
       "      <th>esconder erro</th>\n",
       "      <th>estranha capacidade</th>\n",
       "      <th>...</th>\n",
       "      <th>persistência apesar</th>\n",
       "      <th>que se</th>\n",
       "      <th>se quer</th>\n",
       "      <th>sucesso tem</th>\n",
       "      <th>só existe</th>\n",
       "      <th>tem uma</th>\n",
       "      <th>um êxito</th>\n",
       "      <th>uma estranha</th>\n",
       "      <th>vida que</th>\n",
       "      <th>êxito capacidade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O sucesso tem uma estranha capacidade de escon...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Só existe um êxito: a capacidade de levar a vi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A coragem não é ausência do medo; é a persistê...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Frases  apesar do  ausência do  \\\n",
       "0  O sucesso tem uma estranha capacidade de escon...          0            0   \n",
       "1  Só existe um êxito: a capacidade de levar a vi...          0            0   \n",
       "2  A coragem não é ausência do medo; é a persistê...          1            1   \n",
       "\n",
       "   capacidade de  coragem não  de esconder  de levar  do medo  esconder erro  \\\n",
       "0              1            0            1         0        0              1   \n",
       "1              1            0            0         1        0              0   \n",
       "2              0            1            0         0        2              0   \n",
       "\n",
       "   estranha capacidade  ...  persistência apesar  que se  se quer  \\\n",
       "0                    1  ...                    0       0        0   \n",
       "1                    0  ...                    0       1        1   \n",
       "2                    0  ...                    1       0        0   \n",
       "\n",
       "   sucesso tem  só existe  tem uma  um êxito  uma estranha  vida que  \\\n",
       "0            1          0        1         0             1         0   \n",
       "1            0          1        0         1             0         1   \n",
       "2            0          0        0         0             0         0   \n",
       "\n",
       "   êxito capacidade  \n",
       "0                 0  \n",
       "1                 1  \n",
       "2                 0  \n",
       "\n",
       "[3 rows x 24 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Criar uma matriz de palavras com bigramas\n",
    "text_matriz = pd.DataFrame(term_text_matriz.toarray(), columns=vectorizer_text)\n",
    "\n",
    "#Adicionando as frases na matriz de palavras\n",
    "text_df = pd.DataFrame(text,columns=['Frases'])\n",
    "result = pd.concat([text_df,text_matriz], axis=1)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quanto maior a quantidade de palavras dentro do token, menor a frequência das palavras entre os textos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus: Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Texto no formato de lista\n",
    "text = [\"O sucesso tem uma estranha capacidade de esconder o erro.\",\n",
    "      \"Só existe um êxito: a capacidade de levar a vida que se quer.\",\n",
    "      \"A coragem não é ausência do medo; é a persistência apesar do medo.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Texto no formato de STR\n",
    "\n",
    "Frase = \"Desejo que você não tenha medo da vida, tenha medo de não vivê-la. Não há céu sem tempestades, nem caminhos sem acidentes.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STR -> Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Desejo que você não tenha medo da vida, tenha ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Não há céu sem tempestades, nem caminhos sem a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Frases\n",
       "0  Desejo que você não tenha medo da vida, tenha ...\n",
       "1  Não há céu sem tempestades, nem caminhos sem a..."
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizar a frase em sentenças\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "#1° Converter para lista\n",
    "token1 = sent_tokenize(Frase)\n",
    "\n",
    "#Converter para dataframe\n",
    "tibble1 = pd.DataFrame(token1,columns=['Frases'])\n",
    "tibble1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transformação de um corpus 'Str' para 'Dataframe'**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lista -> Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O sucesso tem uma estranha capacidade de escon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Só existe um êxito: a capacidade de levar a vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A coragem não é ausência do medo; é a persistê...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Frases\n",
       "0  O sucesso tem uma estranha capacidade de escon...\n",
       "1  Só existe um êxito: a capacidade de levar a vi...\n",
       "2  A coragem não é ausência do medo; é a persistê..."
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Transformar a lista em um dataframe\n",
    "tibble2 = pd.DataFrame(text,columns=['Frases'])\n",
    "tibble2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transformação de um corpus 'Lista' para 'Dataframe'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "#formato dos dados\n",
    "print(type(tibble1))\n",
    "print(type(tibble2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizar o dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frases</th>\n",
       "      <th>tokenized_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O sucesso tem uma estranha capacidade de escon...</td>\n",
       "      <td>[O, sucesso, tem, uma, estranha, capacidade, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Só existe um êxito: a capacidade de levar a vi...</td>\n",
       "      <td>[Só, existe, um, êxito, :, a, capacidade, de, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A coragem não é ausência do medo; é a persistê...</td>\n",
       "      <td>[A, coragem, não, é, ausência, do, medo, ;, é,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Frases  \\\n",
       "0  O sucesso tem uma estranha capacidade de escon...   \n",
       "1  Só existe um êxito: a capacidade de levar a vi...   \n",
       "2  A coragem não é ausência do medo; é a persistê...   \n",
       "\n",
       "                                     tokenized_sents  \n",
       "0  [O, sucesso, tem, uma, estranha, capacidade, d...  \n",
       "1  [Só, existe, um, êxito, :, a, capacidade, de, ...  \n",
       "2  [A, coragem, não, é, ausência, do, medo, ;, é,...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenizar \n",
    "tibble2['tokenized_sents']  = tibble2.apply(lambda row: nltk.word_tokenize(row[0]), axis=1)\n",
    "tibble2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Criamos a coluna 'tokenized_sents', que fez a tokenização das palavras de cada frase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frases</th>\n",
       "      <th>tokenized_sents</th>\n",
       "      <th>sents_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O sucesso tem uma estranha capacidade de escon...</td>\n",
       "      <td>[O, sucesso, tem, uma, estranha, capacidade, d...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Só existe um êxito: a capacidade de levar a vi...</td>\n",
       "      <td>[Só, existe, um, êxito, :, a, capacidade, de, ...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A coragem não é ausência do medo; é a persistê...</td>\n",
       "      <td>[A, coragem, não, é, ausência, do, medo, ;, é,...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Frases  \\\n",
       "0  O sucesso tem uma estranha capacidade de escon...   \n",
       "1  Só existe um êxito: a capacidade de levar a vi...   \n",
       "2  A coragem não é ausência do medo; é a persistê...   \n",
       "\n",
       "                                     tokenized_sents  sents_length  \n",
       "0  [O, sucesso, tem, uma, estranha, capacidade, d...            11  \n",
       "1  [Só, existe, um, êxito, :, a, capacidade, de, ...            15  \n",
       "2  [A, coragem, não, é, ausência, do, medo, ;, é,...            15  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Contagem de tokens de palavras por frases\n",
    "tibble2['sents_length'] = tibble2.apply(lambda row: len(row['tokenized_sents']), axis=1)\n",
    "tibble2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Criamos a coluna 'sents_length', que contou a quantidade de tokens de palavras de cada frase**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a               3\n",
       "capacidade      2\n",
       "de              2\n",
       "do              2\n",
       "é               2\n",
       "O               1\n",
       "que             1\n",
       "apesar          1\n",
       "persistência    1\n",
       "medo;           1\n",
       "ausência        1\n",
       "não             1\n",
       "coragem         1\n",
       "A               1\n",
       "quer.           1\n",
       "se              1\n",
       "levar           1\n",
       "vida            1\n",
       "sucesso         1\n",
       "êxito:          1\n",
       "um              1\n",
       "existe          1\n",
       "Só              1\n",
       "erro.           1\n",
       "o               1\n",
       "esconder        1\n",
       "estranha        1\n",
       "uma             1\n",
       "tem             1\n",
       "medo.           1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bag of words \n",
    "bag_df = pd.Series([y for x in tibble2['Frases'].values.flatten() for y in x.split()]).value_counts()\n",
    "bag_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contagem de palavras das frases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "#Transformar o dataframe em uma lista\n",
    "df_list = tibble2['Frases'].tolist()\n",
    "print(type(df_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "#Transformar o dataframe em STR\n",
    "df_str = str(tibble2['Frases'])\n",
    "print(type(df_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus: Dicionário "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'O importante não é vencer todos os dias, mas lutar sempre.',\n",
       " 2: 'É melhor conquistar a si mesmo do que vencer mil batalhas.',\n",
       " 3: 'Quem ousou conquistar e saiu pra lutar, chega mais longe!',\n",
       " 4: 'O medo de perder tira a vontade de ganhar.'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dicionário\n",
    "corpus_dict = {1: 'O importante não é vencer todos os dias, mas lutar sempre.',\n",
    "          2: 'É melhor conquistar a si mesmo do que vencer mil batalhas.',\n",
    "          3: 'Quem ousou conquistar e saiu pra lutar, chega mais longe!',\n",
    "          4: 'O medo de perder tira a vontade de ganhar.'}\n",
    "\n",
    "corpus_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#formato dos dados\n",
    "type(corpus_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Corpus no formato Dicionário tem os atributos chave e valor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['O importante não é vencer todos os dias, mas lutar sempre.', 'É melhor conquistar a si mesmo do que vencer mil batalhas.', 'Quem ousou conquistar e saiu pra lutar, chega mais longe!', 'O medo de perder tira a vontade de ganhar.'])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extrair apenas os valores do dicionário\n",
    "dic_values = corpus_dict.values()\n",
    "dic_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformar o dicionario em uma lista\n",
    "type(list(dic_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformar o dicionario em uma STR\n",
    "type(str(dic_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusão:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**O processamento de linguagem natural garante que o computador reconheça o contexto da mensagem, faça uma análise morfológica, semântica, sintática, e interprete sentidos. Para isso ocorrer, o pré-processamento dos dados tem que ser feito de forma criteriosa, sendo assim, somente os dados importantes serão analisados, e o conhecimento no formato dos dados é essencial, pois ele determina a forma como vamos tratar**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
